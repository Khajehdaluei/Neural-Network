{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b3ec8b6",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;text-align:center\"><img src=\"https://dl.mohammadkh.ir/logo.png\" alt=\"Mohammadkh.ir\" style=\"width: 250px;\"/></div>\n",
    "<h1><div style=\"direction:rtl;text-align:center\">Neural Network</div></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184e4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "#--\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802f3a2b",
   "metadata": {},
   "source": [
    "### create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c04da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['برخیز بتا بیا ز بهر دل ما',\n",
       " 'حل کن به جمال خویشتن مشکل ما',\n",
       " 'یک کوزه شراب تا به هم نوش کنیم',\n",
       " 'زآن پیش که کوزه\\u200cها کنند از گل ما']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ex:\n",
    "## dl page html\n",
    "page = requests.get('https://ganjoor.net/khayyam/robaee/sh1/')\n",
    "print(page, end='\\n\\n')\n",
    "\n",
    "## load tree html\n",
    "tree = html.fromstring(page.content)\n",
    "\n",
    "## get robaee in page\n",
    "tree.xpath('//div[@class=\"b\"]/div/p/text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b9f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get all robaee khayam\n",
    "# for i in range(1, 179):\n",
    "#     page = requests.get('https://ganjoor.net/khayyam/robaee/sh' + str(i) + '/')\n",
    "#     tree = html.fromstring(page.content)\n",
    "#     with open('../../__data/khayyam/khayyam.txt', 'a',encoding='utf8') as f:\n",
    "#         for t in tree.xpath('//div[@class=\"b\"]/div/p/text()'):\n",
    "#             f.write('|' + str(t.replace('\\u200c', ' ')) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27872da",
   "metadata": {},
   "source": [
    "### Load & preprocess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0705f10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(128, 100), dtype=tf.int32, name=None), TensorSpec(shape=(128, 100), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "text = open('../../__data/Text Generator/khayyam_molana.txt', 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# list all vocabolaries\n",
    "vocabolaries = sorted(set(text))\n",
    "# print(vocabolaries)\n",
    "\n",
    "# encoder decoder\n",
    "char2index = {u:i for i, u in enumerate(vocabolaries)}\n",
    "index2char = np.array(vocabolaries)\n",
    "# print(char2index)\n",
    "\n",
    "# replace encoder data\n",
    "text_as_integer = np.array([char2index[c] for c in text])\n",
    "# print(text_as_integer[:5])\n",
    "\n",
    "# create tf.data tensor\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_integer)\n",
    "# for i in char_dataset.take(10):\n",
    "#     print(index2char[i.numpy()])\n",
    "\n",
    "# create tensor input data \n",
    "sequences = char_dataset.batch(101, drop_remainder=True)\n",
    "# for i in sequences.take(3):\n",
    "#     print('--->', ''.join(index2char[i.numpy()]))\n",
    "\n",
    "# inpot split X,Y\n",
    "def sit(batch):\n",
    "    input_text = batch[:-1]\n",
    "    target_text = batch[1:]\n",
    "    return input_text, target_text\n",
    "## map fun\n",
    "dataset = sequences.map(sit)\n",
    "# for i in dataset.take(1):\n",
    "#     print(''.join(index2char[i[0].numpy()]))\n",
    "#     print(''.join(index2char[i[1].numpy()]))\n",
    "\n",
    "# df baching\n",
    "dataset = dataset.batch(128, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f27768",
   "metadata": {},
   "source": [
    "### creat & fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6211320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabolary_size = len(vocabolaries)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb811247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 256)         12032     \n",
      "                                                                 \n",
      " gru (GRU)                   (None, None, 1024)        3938304   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 47)          48175     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,998,511\n",
      "Trainable params: 3,998,511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Embedding(vocabolary_size, embedding_dim),\n",
    "    tf.keras.layers.GRU(rnn_units, return_sequences=True),\n",
    "    tf.keras.layers.Dense(vocabolary_size)\n",
    "    \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "421c1576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'شؤةثآث|خَهوزَبةضی!!ش؟حپصنچخعر!گ|اصکةچت رببق|جءحسنٌذ؟سی!بًترهچَّث\\nحزپجژطٌَسغةژمء\\nزکتشتئنٔ\\nٌئٌحلئحلچض!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model random weight\n",
    "for input_text, target_text in dataset.take(1):\n",
    "    output = model.predict(input_text)\n",
    "    # print(output[0])\n",
    "si = tf.random.categorical(output[0], num_samples=1)\n",
    "''.join(index2char[tf.squeeze(si, axis=-1).numpy()])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f0bac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_f(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fece1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='../../__data/Text Generator/khayyam_molana_.h5',\n",
    "                                                monitor='loss', save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c51253a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 42s 1s/step - loss: 0.1023\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=1, verbose=1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c542b457",
   "metadata": {},
   "source": [
    "### generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8a18b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../../__data/Text Generator/khayyam_molana.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4547a",
   "metadata": {},
   "source": [
    "##### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ea675b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "به نام خداوند جان و خرد\n",
      "ر سینواام ند آان و بو \n",
      "\n",
      " چؤاه ین یمامنن ش آهدب|\n",
      "نهمناشک خکان\n",
      "\n",
      "دیدطسر در\n"
     ]
    }
   ],
   "source": [
    "model.reset_states()\n",
    "# create input generator\n",
    "num_generate = 1000\n",
    "first_string = 'به نام خداوند جان و خرد'\n",
    "input_eval = [char2index[s] for s in first_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "# genereator predict\n",
    "text_generated = ['به نام خداوند جان و خرد']\n",
    "for i in range(3):\n",
    "    # pass input\n",
    "    predictions = model.predict(input_eval)\n",
    "    # expand dim output\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "    \n",
    "    # get random weighted\n",
    "    predicted_ids = tf.random.categorical(predictions, num_samples=1).numpy()\n",
    "    \n",
    "    # expand dim for input\n",
    "    input_eval = tf.expand_dims(tf.squeeze(predicted_ids, axis=-1).numpy(), 0).numpy()\n",
    "    # add to output generator\n",
    "    text_generated.append(index2char[tf.squeeze(predicted_ids, axis=-1).numpy()])\n",
    "\n",
    "    \n",
    "for i in text_generated:\n",
    "    print(''.join(i))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe894159",
   "metadata": {},
   "source": [
    "##### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c529e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "به نام خداوند جان و خرد\n",
      "|جمله است این هر دوان کشت از بهر غباب\n",
      "|از برای حق که گاو در ملا\n",
      "|کان بخواهد سوی محسوسات ژخل\n",
      "|گه تنامض لا چو پرهانی بود\n",
      "|چون بمودی هیچ سگ می روی تو\n",
      "|وز همر از لقمه مسکن خمر\n",
      "|درد او پیدا و دست و او ز خاک\n",
      "|نه کمان تاویل بر گردانیان\n",
      "|بعد از آن گفتش سلام التقوی است\n",
      "|لاجرم بندند پیش از قصر ما\n",
      "|احمدا این کار پیشینی برون\n",
      "|شد شقی و خوبیات او یک خانه باش\n",
      "|دست و پاهاشان گواهی می دهند\n",
      "|تا نماید بار با خود گرد تاست آورد\n",
      "|ناپذیرف نیست کرد آن شهرها\n",
      "|نطق کو بدزاده لا محلوم باش\n",
      "|گوش را بگذار و کی آلود باشد\n",
      "|چون\n"
     ]
    }
   ],
   "source": [
    "model.reset_states()\n",
    "# create input generator\n",
    "num_generate = 1000\n",
    "first_string = 'به نام خداوند جان و خرد'\n",
    "input_eval = [char2index[s] for s in first_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "# genereator predict\n",
    "text_generated = ['به نام خداوند جان و خرد']\n",
    "for i in range(500):\n",
    "    # pass input\n",
    "    predictions = model.predict(input_eval)\n",
    "    # expand dim output\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "    \n",
    "    # get random weighted --> just firest work\n",
    "    predicted_ids = tf.random.categorical(predictions, num_samples=1).numpy().reshape(-1, 1)[-1][0]\n",
    "    \n",
    "    # append to message and create new input\n",
    "    message = np.append(input_eval[0].numpy(), predicted_ids)[1:]\n",
    "    # expand dim for input\n",
    "    input_eval = tf.expand_dims(message, 0)\n",
    "    # add to output generator\n",
    "    text_generated.append(index2char[predicted_ids])\n",
    "\n",
    "    \n",
    "for i in ''.join(text_generated).split('\\n'):\n",
    "    print(i)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96529ae1",
   "metadata": {},
   "source": [
    "##### V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94eddc5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "به نام خداوند جان و خرد\n",
      "|جمله مرغان را چو زندانی بود\n",
      "|چون بدر که عقل خود را در کشید\n",
      "|تا زنده آن کاری که در دل شک نظر\n",
      "|با در تا می گزارد هیچ خشم\n",
      "|زشم شاهنشه که مدخل ای فلان\n",
      "|من بگویم رست از سوی جهان\n",
      "|آن دو دزدانیم ابلهست این را ز شرق\n",
      "|کیست یک خود کی بر آن را منتظر\n",
      "|با دل موسی مگه کلی از شکر و صد\n",
      "|چشم آخربین بسسته می شوی گویی تو صد\n",
      "|همچو اندر شیر خاکی می برد\n",
      "|من نکرد او ملک هفت زنگ\n",
      "|این جهان نفس و در آمد نه ز نان\n",
      "|نیست خاده کرده قودی حال او\n",
      "|باز گرد امروز فهم باداری\n",
      "|تا چند کناره و ناپیدا شنو\n",
      "|این چنین شه نام تستیح اندر\n"
     ]
    }
   ],
   "source": [
    "model.reset_states()\n",
    "# create input generator\n",
    "num_generate = 1000\n",
    "first_string = 'به نام خداوند جان و خرد'\n",
    "input_eval = [char2index[s] for s in first_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "# genereator predict\n",
    "text_generated = ['به نام خداوند جان و خرد']\n",
    "for i in range(500):\n",
    "    # pass input\n",
    "    predictions = model.predict(input_eval)\n",
    "    # expand dim output\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "    \n",
    "    # get argmax just firest work\n",
    "    predicted_ids = np.array(predictions.numpy()).argmax(axis=1).reshape(-1, 1)[-1][0]\n",
    "    \n",
    "    # append to message and create new input\n",
    "    message = np.append(input_eval[0].numpy(), predicted_ids)[1:]\n",
    "    # expand dim for input\n",
    "    input_eval = tf.expand_dims(message, 0)\n",
    "    # add to output generator\n",
    "    text_generated.append(index2char[predicted_ids])\n",
    "\n",
    "    \n",
    "for i in ''.join(text_generated).split('\\n'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860432d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<div style=\"direction:rtl;text-align:left\"><strong>Neural Network</strong><br>MohammadReza <strong>Khajedaloi</strong><br><br>\n",
    "</div>\n",
    "<div style=\"direction:rtl;text-align:right\">\n",
    "<a href=\"http://mohammadkh.ir/\">WebSite</a> - <a href=\"https://github.com/khajedaloi/\">GitHub</a> - <a href=\"https://www.linkedin.com/in/mohammad-kh/\">Linkedin</a>\n",
    "</div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep_"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
