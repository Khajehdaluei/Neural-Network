{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b3ec8b6",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;text-align:center\"><img src=\"https://mohammadkh.ir/github/logo.png\" alt=\"Mohammadkh.ir\" style=\"width: 250px;\"/></div>\n",
    "<h1><div style=\"direction:rtl;text-align:center\">Neural Network</div></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc984399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f063b",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53e7e573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.recurrent_v2.LSTM at 0x1e091878460>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM(\n",
    "    units=128,                            # number noron lstm\n",
    "    activation=\"tanh\",                    # relu better but not support gpu\n",
    "    return_sequences=False,               # output all noron if True\n",
    "    return_state=False,                   # True --> output + last state\n",
    "    go_backwards=False,                   # if go_backwards: --> inputs = reverse(inputs, 0)\n",
    "    stateful=False,                       # save state inloop --> but bachsizi input layer\n",
    "    unroll=False,                         # faster process --> cash process memmory --> but not support gpu\n",
    "    recurrent_activation=\"sigmoid\",       # dont touch :D \n",
    "    # Layer weight customize -->\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    recurrent_initializer=\"orthogonal\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    unit_forget_bias=True,\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    time_major=False,                     # True --> [timesteps, batch, feature], False -->  [batch, timesteps, feature]\n",
    "    # dropout --> not good\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79523318",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280b8029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.recurrent_v2.GRU at 0x1e091899ee0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GRU(\n",
    "    units=128,                            # number noron lstm\n",
    "    activation=\"tanh\",                    # relu better but not support gpu\n",
    "    return_sequences=False,               # output all noron if True\n",
    "    return_state=False,                   # True --> output + last state\n",
    "    go_backwards=False,                   # if go_backwards: --> inputs = reverse(inputs, 0)\n",
    "    stateful=False,                       # save state inloop --> but bachsizi input layer\n",
    "    unroll=False,                         # faster process --> cash process memmory --> but not support gpu\n",
    "    recurrent_activation=\"sigmoid\",       # dont touch :D \n",
    "    # Layer weight customize -->\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    recurrent_initializer=\"orthogonal\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    # dropout --> not good\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    time_major=False,                     # True --> [timesteps, batch, feature], False -->  [batch, timesteps, feature]\n",
    "    reset_after=True,                     # dont tuch --> False = \"before\", True = \"after\" (default and cuDNN compatible)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f2f0ad",
   "metadata": {},
   "source": [
    "## SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2613198d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.recurrent.SimpleRNN at 0x1e091878c70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SimpleRNN(\n",
    "    units=128,                            # number noron lstm\n",
    "    activation=\"tanh\",                    # relu better but not support gpu\n",
    "    return_sequences=False,               # output all noron if True\n",
    "    return_state=False,                   # True --> output + last state\n",
    "    go_backwards=False,                   # if go_backwards: --> inputs = reverse(inputs, 0)\n",
    "    stateful=False,                       # save state inloop --> but bachsizi input layer\n",
    "    unroll=False,                         # faster process --> cash process memmory --> but not support gpu\n",
    "    # Layer weight customize -->\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    recurrent_initializer=\"orthogonal\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    # dropout --> not good    \n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045b6ac1",
   "metadata": {},
   "source": [
    "## TimeDistributed\n",
    "### share wighte layer sequences\n",
    "### convert normlayer to Recurrent layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8b9e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 10, 60) dtype=float32 (created by layer 'time_distributed')>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(10, 128))\n",
    "layer = Dense(60)\n",
    "\n",
    "TimeDistributed(layer)(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a91d2a",
   "metadata": {},
   "source": [
    "## Bidirectional      (forward <---> backward)\n",
    "### LSTM or GRU\n",
    "##### support --> go_backwards, return_sequences and return_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a15c342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.wrappers.Bidirectional at 0x1e091f29610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bidirectional(\n",
    "    layer=GRU(units=128),                              # forward layer\n",
    "    backward_layer=LSTM(10, go_backwards=True),        # backward layer\n",
    "    merge_mode=\"concat\",\n",
    "    # {'sum', 'mul', 'concat', 'ave', None}. If None, the outputs will not be combined, they will be returned as a list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826649b0",
   "metadata": {},
   "source": [
    "## ConvLSTM\n",
    "### All parameters are the same --->  1D - 2D - 3D "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539a47c",
   "metadata": {},
   "source": [
    "### ConvLSTM1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ce9e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional_recurrent.ConvLSTM1D at 0x1e091f5d610>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvLSTM1D(\n",
    "    filters=64,                           # number kernel\n",
    "    kernel_size=7,                        # shape kernel\n",
    "    padding=\"valid\",                      # \"valid\" or \"same\"\n",
    "    strides=1,                            # skip kernel \n",
    "    dilation_rate=1,                      # skip noron \n",
    "    return_sequences=False,               # output all noron if True\n",
    "    return_state=False,                   # True --> output + last state\n",
    "    go_backwards=False,                   # if go_backwards: --> inputs = reverse(inputs, 0)\n",
    "    stateful=False,                       # save state inloop --> but bachsizi input layer\n",
    "    activation=\"tanh\",                    # relu better \n",
    "    recurrent_activation=\"hard_sigmoid\",  # dont touch :D \n",
    "    # Layer weight customize -->\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    recurrent_initializer=\"orthogonal\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    unit_forget_bias=True,\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    # dropout --> not good\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    data_format=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c6d9a",
   "metadata": {},
   "source": [
    "### ConvLSTM2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc6ddc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional_recurrent.ConvLSTM2D at 0x1e09195d580>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvLSTM2D(\n",
    "    filters=64,                           # number kernel\n",
    "    kernel_size=(7,7),                    # shape kernel\n",
    "    padding=\"valid\",                      # \"valid\" or \"same\"\n",
    "    strides=(1, 1),                       # skip kernel \n",
    "    dilation_rate=(1, 1),                 # skip noron \n",
    "    return_sequences=False,               # output all noron if True\n",
    "    return_state=False,                   # True --> output + last state\n",
    "    go_backwards=False,                   # if go_backwards: --> inputs = reverse(inputs, 0)\n",
    "    stateful=False,                       # save state inloop --> but bachsizi input layer\n",
    "    activation=\"tanh\",                    # relu better \n",
    "    recurrent_activation=\"hard_sigmoid\",  # dont touch :D \n",
    "    # Layer weight customize -->\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    recurrent_initializer=\"orthogonal\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    unit_forget_bias=True,\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    # dropout --> not good\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    data_format=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b885c9c1",
   "metadata": {},
   "source": [
    "### ConvLSTM3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1e09299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional_recurrent.ConvLSTM3D at 0x1e091f5dd60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvLSTM3D(\n",
    "    filters=64,                           # number kernel\n",
    "    kernel_size=(7, 7, 7),                # shape kernel\n",
    "    padding=\"valid\",                      # \"valid\" or \"same\"\n",
    "    strides=(1, 1, 1),                    # skip kernel \n",
    "    dilation_rate=(1, 1, 1),              # skip noron \n",
    "    return_sequences=False,               # output all noron if True\n",
    "    return_state=False,                   # True --> output + last state\n",
    "    go_backwards=False,                   # if go_backwards: --> inputs = reverse(inputs, 0)\n",
    "    stateful=False,                       # save state inloop --> but bachsizi input layer\n",
    "    activation=\"tanh\",                    # relu better \n",
    "    recurrent_activation=\"hard_sigmoid\",  # dont touch :D \n",
    "    # Layer weight customize -->\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    recurrent_initializer=\"orthogonal\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    unit_forget_bias=True,\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    # dropout --> not good\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    data_format=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860432d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<div style=\"direction:rtl;text-align:left\"><strong>Neural Network</strong><br>MohammadReza <strong>Khajedaloi</strong><br><br>\n",
    "</div>\n",
    "<div style=\"direction:rtl;text-align:right\">\n",
    "<a href=\"http://mohammadkh.ir/\">WebSite</a> - <a href=\"https://github.com/khajedaloi/\">GitHub</a> - <a href=\"https://www.linkedin.com/in/mohammad-kh/\">Linkedin</a>\n",
    "</div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep_"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
